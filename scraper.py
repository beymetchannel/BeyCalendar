import json
import re
import os
# --- â˜…ã“ã“ã‚’ä¿®æ­£/ç¢ºèªã—ã¦ãã ã•ã„â˜… ---
from playwright.sync_api import sync_playwright 
# ------------------------------------
from bs4 import BeautifulSoup
    
URL = "https://beyblade.takaratomy.co.jp/beyblade-x/event/schedule.html#schedule"

def get_color_class(event_type):
    # ... (ã“ã®é–¢æ•°ã¯å¤‰æ›´ãªã—) ...
    if "G3å¤§ä¼šï¼ˆãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼" in event_type or "ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹" in event_type:
        return 'G3(R)'
    elif "G3å¤§ä¼šï¼ˆã‚ªãƒ¼ãƒ—ãƒ³" in event_type or "ã‚ªãƒ¼ãƒ—ãƒ³ã‚¯ãƒ©ã‚¹" in event_type:
        return 'G3(O)'
    # ... (ãã®ä»–ã®ãƒ­ã‚¸ãƒƒã‚¯) ...
    else:
        return 'ãã®ä»–'

def scrape_beyblade_events_dynamic():
    """Playwrightã‚’ä½¿ç”¨ã—ã¦å‹•çš„ã«èª­ã¿è¾¼ã¾ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹"""
    events_data = []
    
    # ğŸ’¡ Playwrightã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨å®Ÿè¡Œ
    with sync_playwright() as p:
        try:
            # GitHub Actionsç’°å¢ƒã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã«'chromium'ã‚’ä½¿ç”¨
            browser = p.chromium.launch()
            page = browser.new_page()
            
            # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            # ä¿®æ­£å‰:
            # page.goto(URL, wait_until="networkidle") 
            
            # ä¿®æ­£å¾Œ: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ã‚’60ç§’ã«å»¶é•·ã—ã€å¾…æ©Ÿæ¡ä»¶ã‚’ "domcontentloaded" ã«ç·©å’Œ
            print("Navigating with longer timeout...")
            page.goto(URL, wait_until="domcontentloaded", timeout=60000) # 60ç§’å¾…æ©Ÿ
            
            # ğŸ’¡ ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆè¦ç´ ãŒå‡ºç¾ã™ã‚‹ã®ã‚’æ˜ç¤ºçš„ã«å¾…æ©Ÿ
            # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å¾Œã€å‹•çš„è¦ç´ ï¼ˆ.event-list-itemï¼‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…ã¤
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„
            print("Waiting for dynamic content to load...")
            page.wait_for_selector('div.event-list-item', timeout=30000) 
            
            # å®Œå…¨ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸHTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            content = page.content()
            
            browser.close()

            # Beautiful Soupã§HTMLã‚’è§£æ
            soup = BeautifulSoup(content, 'html.parser')
            event_elements = soup.find_all('div', class_='event-list-item')
            
            # ... (ã“ã“ã‹ã‚‰BeautifulSoupã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã¯å‰å›ã¨åŒæ§˜) ...
            
            for item in event_elements:
                try:
                    date_time_str = item.find('p', class_='date-time').text.strip()
                    date_match = re.search(r'(\d{4}å¹´\s*\d{1,2}æœˆ\d{1,2}æ—¥)', date_time_str)
                    time_match = re.search(r'(\d{1,2}ï¼š\d{2})', date_time_str)
                    
                    date_str = date_match.group(1).strip() if date_match else "æ—¥ä»˜ä¸æ˜"
                    time_str = time_match.group(1).strip() if time_match else "æ™‚é–“ä¸æ˜"

                    event_type = item.find('p', class_='event-name').text.strip()
                    name_location = item.find('p', class_='name-location').text.strip()
                    address = item.find('p', class_='address').text.strip()
                    details = ' '.join([p.text.strip() for p in item.find_all('p', class_='text-style-01')]).replace('\n', ' ')

                    events_data.append({
                        "date": date_str,
                        "time": time_str,
                        "type": event_type,
                        "name": name_location,
                        "location": name_location,
                        "address": address,
                        "details": details,
                        "color_label": get_color_class(event_type) 
                    })
                    
                except AttributeError as e:
                    print(f"Skipping event due to missing tag: {e}")
            
            return events_data

        except Exception as e:
            print(f"Playwright execution error: {e}")
            return []

def save_data(data):
    """ãƒ‡ãƒ¼ã‚¿ã‚’data/events.jsonã«ä¿å­˜ã™ã‚‹"""
    os.makedirs('data', exist_ok=True)
    with open('data/events.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"Successfully saved {len(data)} events to data/events.json")


if __name__ == "__main__":
    # âŒ èª¤: extracted_data = scrape_beyblade_events()
    # âœ… æ­£: ä¿®æ­£å¾Œã®é–¢æ•°å scrape_beyblade_events_dynamic ã‚’å‘¼ã³å‡ºã™
    extracted_data = scrape_beyblade_events_dynamic() 
    if extracted_data:
        save_data(extracted_data)


if __name__ == "__main__":
    extracted_data = scrape_beyblade_events()
    if extracted_data:
        save_data(extracted_data)
